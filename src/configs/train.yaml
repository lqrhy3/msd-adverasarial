run_name: test_run
num_epochs: 10
batch_size: 2
#data_path: ~/data
data_dir: ~/PycharmProjects/coursework/data/raw
cache_num: 0

transform:
  train:
    compose_fn:
      type: monai.transforms.Compose
    transform_fns:
      - type: monai.transforms.LoadImaged
        keys: [image, label]

      - type: monai.transforms.EnsureChannelFirstd
        keys: [image, label]

      - type: monai.transforms.Spacingd
        # values from https://github.com/NVIDIA/DeepLearningExamples/blob/ddbcd54056e8d1bc1c4d5a8ab34cb570ebea1947/PyTorch/Segmentation/nnUNet/data_preprocessing/configs.py#L62
        keys: [image, label]
        pixdim: [0.79, 0.79, 1.24]
        mode: [bilinear, nearest]

      - type: src.utils.transforms.ClipIntensityd
        # values from https://github.com/NVIDIA/DeepLearningExamples/blob/ddbcd54056e8d1bc1c4d5a8ab34cb570ebea1947/PyTorch/Segmentation/nnUNet/data_preprocessing/configs.py#L81
        keys: [image]
        i_min: -1024
        i_max: 325

      - type: monai.transforms.NormalizeIntensityd
        # values from https://github.com/NVIDIA/DeepLearningExamples/blob/ddbcd54056e8d1bc1c4d5a8ab34cb570ebea1947/PyTorch/Segmentation/nnUNet/data_preprocessing/configs.py#L99
        keys: [image]
        subtrahend: -158.58
        divisor: 324.7

      - type: monai.transforms.RandCropByPosNegLabeld
        keys: [image, label]
        label_key: label
#        spatial_size: [256, 256, 126]
        spatial_size: [64, 64, 32]
        pos: 2
        neg: 1
        num_samples: 3

      - type: monai.transforms.ToTensord
        keys: [image, label]

  val:
    compose_fn:
      type: monai.transforms.Compose
    transform_fns:
      - type: monai.transforms.LoadImaged
        keys: [image, label]

      - type: monai.transforms.EnsureChannelFirstd
        keys: [image, label]

      - type: monai.transforms.Spacingd
        # values from https://github.com/NVIDIA/DeepLearningExamples/blob/ddbcd54056e8d1bc1c4d5a8ab34cb570ebea1947/PyTorch/Segmentation/nnUNet/data_preprocessing/configs.py#L62
        keys: [image, label]
        pixdim: [0.79, 0.79, 1.24]
        mode: [bilinear, nearest]

      - type: src.utils.transforms.ClipIntensityd
        # values from https://github.com/NVIDIA/DeepLearningExamples/blob/ddbcd54056e8d1bc1c4d5a8ab34cb570ebea1947/PyTorch/Segmentation/nnUNet/data_preprocessing/configs.py#L81
        keys: [image]
        i_min: -1024
        i_max: 325

      - type: monai.transforms.NormalizeIntensityd
        # values from https://github.com/NVIDIA/DeepLearningExamples/blob/ddbcd54056e8d1bc1c4d5a8ab34cb570ebea1947/PyTorch/Segmentation/nnUNet/data_preprocessing/configs.py#L99
        keys: [image]
        subtrahend: -158.58
        divisor: 324.7

      - type: monai.transforms.ToTensord
        keys: [image, label]
